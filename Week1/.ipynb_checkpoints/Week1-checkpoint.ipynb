{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization:\n",
    "* Discrete optimization : Dynamic programming\n",
    "* Continue optimization : Gradient Descent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic programming: \n",
    "# Recursive+memory-> cache [memory problem]:\n",
    "\n",
    "'''How many times needed to edimt both strings to the same!'''\n",
    "\n",
    "# Assumption:\n",
    "# only acts on s1\n",
    "\n",
    "def editDistance(s1,s2):\n",
    "    # recursive function:\n",
    "    cache={}\n",
    "    def edit(m,n):\n",
    "        if (m,n) in cache:\n",
    "            # if in cache alr then return \n",
    "            return cache[(m,n)]\n",
    "        if m==0:\n",
    "            result = n\n",
    "        elif n==0:\n",
    "            result =  m\n",
    "        elif s1[m-1]==s2[n-1]:\n",
    "            result = edit(m-1,n-1)\n",
    "        \n",
    "        else:\n",
    "            deletion=1+edit(m-1,n)\n",
    "            add=1+edit(m,n-1)\n",
    "            change=1+edit(m-1,n-1)\n",
    "            result =  min(deletion,add,change)\n",
    "            \n",
    "        cache[(m, n)] = result    \n",
    "        return result\n",
    "    return edit(len(s1),len(s2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editDistance('a cat'*10,'the cats'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent :\n",
    "import numpy as np\n",
    "# Generate artificial data:\n",
    "wt=np.array([1,2,3,4])\n",
    "d=len(wt)\n",
    "Tdata=[]\n",
    "for i in range(10000):\n",
    "    x=np.random.randn(d)\n",
    "    y=wt.dot(x)+np.random.randn() # adding random noisy \n",
    "    Tdata.append((x,y))\n",
    "\n",
    "# Loss function for gd:\n",
    "def Loss(w):\n",
    "    return sum((w.dot(x)-y)**2 for x,y in Tdata)/len(Tdata)\n",
    "# Derivative of Loss function:\n",
    "def dLoss(w):\n",
    "    return sum(2*(w.dot(x)-y)*x for x,y in Tdata)/len(Tdata)\n",
    "\n",
    "# Loss function for sgd:\n",
    "def sLoss(w,i):\n",
    "    x,y=Tdata[i]\n",
    "    return (w.dot(x)-y)**2\n",
    "def sdLoss(w,i):\n",
    "    x,y=Tdata[i]\n",
    "    return 2*(w.dot(x)-y)*x\n",
    "\n",
    "# Gradient descent :\n",
    "def gradient_descent(Loss,dLoss,d):\n",
    "    # initialize w:\n",
    "    w=np.zeros(d)\n",
    "    eta=0.01\n",
    "    for i in range(101):\n",
    "        w=w-eta*dLoss(w)\n",
    "        if i%10 ==0:\n",
    "            print('{}iteration: Loss:{},W:{}'.format(i,Loss(w),w))\n",
    "\n",
    "# Stochastic gradient descent:\n",
    "def S_gradient_descent(Loss,dLoss,d):\n",
    "    # initiate:\n",
    "    w=np.zeros(d)\n",
    "    # using decreasing eta\n",
    "    eta=1\n",
    "    num=0\n",
    "    for i in range(101):\n",
    "       \n",
    "        for j in range(len(Tdata)):\n",
    "            num+=1\n",
    "            eta=1/num\n",
    "            w=w-eta*dLoss(w,i)\n",
    "        \n",
    "        if i%10 ==0:\n",
    "            print('{}iteration: Loss:{},W:{}'.format(i,Loss(w,i),w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0iteration: Loss:30.02318134450995,W:[0.01950375 0.04009624 0.06085574 0.07999426]\n",
      "10iteration: Loss:20.375453556652847,W:[0.19498276 0.39936604 0.60595341 0.79710846]\n",
      "20iteration: Loss:13.936318210778134,W:[0.33937073 0.69271943 1.05075929 1.38318138]\n",
      "30iteration: Loss:9.638640862009408,W:[0.45817057 0.9322539  1.41372714 1.86215742]\n",
      "40iteration: Loss:6.770213918447867,W:[0.55591235 1.12784607 1.70991498 2.25360601]\n",
      "50iteration: Loss:4.855704521260416,W:[0.636325   1.2875595  1.95161    2.57352094]\n",
      "60iteration: Loss:3.5778690891987863,W:[0.70247785 1.41797763 2.14883847 2.83497362]\n",
      "70iteration: Loss:2.724973131439261,W:[0.75689719 1.52447552 2.30978177 3.04864703]\n",
      "80iteration: Loss:2.1556997411524152,W:[0.8016622  1.61144174 2.4411159  3.22327216]\n",
      "90iteration: Loss:1.7757296600594832,W:[0.83848401 1.68245947 2.54828848 3.3659846 ]\n",
      "100iteration: Loss:1.5221107679703003,W:[0.86877078 1.74045432 2.6357448  3.48261606]\n"
     ]
    }
   ],
   "source": [
    "gradient_descent(Loss, dLoss, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0iteration: Loss:3.0814879110195774e-33,W:[ 0.06398265 -0.02264606  0.01363084 -0.01568007]\n",
      "10iteration: Loss:0.6055430348988643,W:[1.00612357 1.5212979  2.44421655 4.00703272]\n",
      "20iteration: Loss:0.008694452851759188,W:[0.73949884 1.98432871 2.98198121 4.37482068]\n",
      "30iteration: Loss:1.0520894635944544,W:[0.83460743 2.08117752 2.82128481 4.25562682]\n",
      "40iteration: Loss:1.1562974541661337,W:[0.72053811 2.02462164 3.01022737 4.17022214]\n",
      "50iteration: Loss:0.39178173643246045,W:[0.70031991 1.97495147 3.02139324 4.16010792]\n",
      "60iteration: Loss:0.15356232432706673,W:[0.77716326 2.03094299 2.94429759 4.19244545]\n",
      "70iteration: Loss:1.7507842117022072,W:[0.80559237 1.78454256 2.94974323 4.24620357]\n",
      "80iteration: Loss:2.326760896817519,W:[0.97117734 1.79282607 2.87383726 4.23851195]\n",
      "90iteration: Loss:0.8558958060063686,W:[0.97830332 1.84211432 2.93532451 4.10247798]\n",
      "100iteration: Loss:0.47661491649274496,W:[0.98706837 1.87462573 3.05768804 4.10085097]\n"
     ]
    }
   ],
   "source": [
    "S_gradient_descent(sLoss, sdLoss, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
